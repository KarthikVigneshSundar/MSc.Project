{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cad694b-d36e-4b85-a0b6-e03f1ad7ec01",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940bc869-c835-4538-b43d-af7fcbda4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openbiolink.obl2021 import OBL2021Dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch.utils.data\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f9f09-aebf-474c-b1c7-93dff25e026d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Accessing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd940b1b-da8d-4968-a340-d3a162b7ed7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found in D:\\programming repo-1\\university of glasgow\\msc project\\MSc.Project\\obl2021, omitting download...\n"
     ]
    }
   ],
   "source": [
    "obl_dataset = OBL2021Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5b5355-08ff-4e87-848f-49695a6d0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training, validation and test sets\n",
    "\n",
    "#obl_train_dataset = obl_dataset.training\n",
    "#obl_valid_dataset = obl_dataset.validation\n",
    "#obl_test_dataset = obl_dataset.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54eeb4ec-9039-4f85-99f1-fd561037c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training tensor is :  torch.Size([4192002, 3])\n",
      "The shape of the validation tensor is :  torch.Size([186301, 3])\n",
      "The shape of the testing tensor is :  torch.Size([180964, 3])\n",
      "The data type of the training tensor is :  torch.int64\n",
      "The data type of the validation tensor is :  torch.int64\n",
      "The data type of the testing tensor is :  torch.int64\n"
     ]
    }
   ],
   "source": [
    "#checking out the tensor shape\n",
    "print('The shape of the training tensor is : ', obl_dataset.training.shape)\n",
    "print('The shape of the validation tensor is : ', obl_dataset.validation.shape)\n",
    "print('The shape of the testing tensor is : ', obl_dataset.testing.shape)\n",
    "\n",
    "#checking out the tensor dtype\n",
    "print('The data type of the training tensor is : ', obl_dataset.training.dtype)\n",
    "print('The data type of the validation tensor is : ', obl_dataset.validation.dtype)\n",
    "print('The data type of the testing tensor is : ', obl_dataset.testing.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40f15b-19fa-48f0-9d5f-93190d611905",
   "metadata": {},
   "source": [
    "# Embedding Models:\n",
    "\n",
    "## TransE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ecc24a83-2056-40b6-90e4-98ecda932329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(nn.Module):\n",
    "    def __init__(self, device, num_entity, num_relation, emb_dim, gamma):\n",
    "        super(TransE, self).__init__()\n",
    "        self.device = device\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_entity = num_entity\n",
    "        self.num_relation = num_relation\n",
    "        \n",
    "        #initialize entity embeddings\n",
    "        self.entity_emb = self.initialize_emb(self.num_entity, self.emb_dim)\n",
    "\n",
    "        #initialie relation embeddings\n",
    "        self.relation_emb = self.initialize_emb(self.num_relation, self.emb_dim)\n",
    "        self.relation_emb.weight.data.div_(self.relation_emb.weight.data.norm(p=2, dim=1, keepdim=True))\n",
    "        #create the loss function\n",
    "        self.loss_fn = nn.MarginRankingLoss(margin=gamma)\n",
    "        \n",
    "\n",
    "    def initialize_emb(self, num_emb, emb_dim):\n",
    "        emb_weight_range = 6 / math.sqrt(emb_dim)\n",
    "        emb = nn.Embedding(num_embeddings=num_emb, embedding_dim=emb_dim, device=self.device)\n",
    "        emb.weight.data.uniform_( -emb_weight_range, emb_weight_range )\n",
    "        return emb\n",
    "    \n",
    "    def forward(self, pos_triplet, neg_triplet):\n",
    "        self.entity_emb.weight.data.div_(self.entity_emb.weight.data.norm(p=2, dim=1, keepdim=True))\n",
    "        pos_distance = self.cal_distance(pos_triplet)\n",
    "        neg_distance = self.cal_distance(neg_triplet)\n",
    "        return self.loss_fn(pos_distance, neg_distance, torch.tensor([[-1]]*512, dtype=torch.int64, device=self.device))\n",
    "    \n",
    "    def cal_distance(self, triplet):\n",
    "        head = triplet[:,0]\n",
    "        relation = triplet[:,1]\n",
    "        tail = triplet[:,2]\n",
    "        #print(head.shape)\n",
    "        return (self.entity_emb(head) + self.relation_emb(relation) - self.entity_emb(tail))\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "76939c0b-2475-4fd3-9792-e5b54fd7947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test transe_model, remove later\n",
    "#transe_model = TransE(device = 'cuda', num_entity = 100, num_relation = 10, emb_dim = 100, gamma = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21841d53-0e13-4625-b71a-d3e00ce60c47",
   "metadata": {},
   "source": [
    "### Create Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "34f8f044-b2a9-4262-81b3-e91a0f03f9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncount = 0\\nfor i in dataset:\\n    print(i)\\n    count +=1\\n    if count == 10:\\n        break\\n'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Create_dataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.size = data.size()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n",
    "    \n",
    "    def size(self):\n",
    "        return self.size\n",
    "\n",
    "obl_train_dataset = TensorDataset(obl_dataset.training)\n",
    "obl_valid_dataset = TensorDataset(obl_dataset.validation)\n",
    "obl_test_dataset = TensorDataset(obl_dataset.testing)\n",
    "\n",
    "dataset = Create_dataset(obl_dataset.training)\n",
    "\n",
    "print(type(obl_dataset.training.shape[0]))\n",
    "print(obl_dataset.training.size())\n",
    "'''\n",
    "\n",
    "'''\n",
    "count = 0\n",
    "for i in dataset:\n",
    "    print(i)\n",
    "    count +=1\n",
    "    if count == 10:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7dc49-808b-40e7-b9d7-8d335d25a74b",
   "metadata": {},
   "source": [
    "### Create DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dd42b2b6-c9a5-4fe2-af05-e58da2572646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size, shuffle):\n",
    "    tensor_dataset = TensorDataset(dataset)\n",
    "    return DataLoader(tensor_dataset, batch_size=batch_size, shuffle=shuffle) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e85af-3bde-4bf8-ae58-c32450a0ba7b",
   "metadata": {},
   "source": [
    "### Training Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "90e27852-349f-4bf9-ab9c-a86affeb3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transe(model, data_loader, optimizer, epoch, device):\n",
    "    for i in range(epoch):\n",
    "        for index, batch_data in enumerate(data_loader):  \n",
    "            #print(len(batch_data))\n",
    "            #print(batch_data[0][0])\n",
    "            #break\n",
    "            optimizer.zero_grad()\n",
    "            sample_data = batch_data[0].to(device)\n",
    "            corr_sample_data = None\n",
    "            loss = model(sample_data, sample_data)\n",
    "            loss.mean().backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7787801-6f53-47dc-a75b-19f9f65ba1ae",
   "metadata": {},
   "source": [
    "### Evaluation Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ec33ed26-2607-4155-b84c-308bbfb7f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52220e15-5812-4db8-b42e-be7f906e798b",
   "metadata": {},
   "source": [
    "### Set parameters and call train TransE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "14c6eecd-061f-4fdd-abf7-2fa3ceb0f5bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (258) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [123]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(transe_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      4\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m create_dataloader(obl_dataset\u001b[38;5;241m.\u001b[39mtraining, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain_transe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranse_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [115]\u001b[0m, in \u001b[0;36mtrain_transe\u001b[1;34m(model, data_loader, optimizer, epoch, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m sample_data \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m corr_sample_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [122]\u001b[0m, in \u001b[0;36mTransE.forward\u001b[1;34m(self, pos_triplet, neg_triplet)\u001b[0m\n\u001b[0;32m     27\u001b[0m pos_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcal_distance(pos_triplet)\n\u001b[0;32m     28\u001b[0m neg_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcal_distance(neg_triplet)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1323\u001b[0m, in \u001b[0;36mMarginRankingLoss.forward\u001b[1;34m(self, input1, input2, target)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1: Tensor, input2: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmargin_ranking_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python38\\lib\\site-packages\\torch\\nn\\functional.py:3319\u001b[0m, in \u001b[0;36mmargin_ranking_loss\u001b[1;34m(input1, input2, target, margin, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (input1\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m input2\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mor\u001b[39;00m input1\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mdim()):\n\u001b[0;32m   3313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   3314\u001b[0m         (\n\u001b[0;32m   3315\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmargin_ranking_loss : All input tensors should have same dimension but got sizes: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3316\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput1: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, input2: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, target: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(input1\u001b[38;5;241m.\u001b[39msize(), input2\u001b[38;5;241m.\u001b[39msize(), target\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3317\u001b[0m         )\n\u001b[0;32m   3318\u001b[0m     )\n\u001b[1;32m-> 3319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmargin_ranking_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (258) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "transe_model = TransE(device = 'cuda', num_entity = 184732, num_relation = 28, emb_dim = 100, gamma = 0.01)\n",
    "optimizer = torch.optim.Adam(transe_model.parameters(), lr=0.0001)\n",
    "\n",
    "data_loader = create_dataloader(obl_dataset.training, batch_size=512, shuffle=True)\n",
    "train_transe(model=transe_model, data_loader=data_loader, optimizer=optimizer, epoch=20, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f820fee-ade0-41a8-b1e9-d00de90ceee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f769e26-2623-4590-952d-66272ca1307a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
